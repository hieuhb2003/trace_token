# vLLM API vá»›i Langfuse Integration

Há»‡ thá»‘ng nÃ y cung cáº¥p 2 API vLLM riÃªng biá»‡t cháº¡y trÃªn 2 GPU khÃ¡c nhau, tÃ­ch há»£p vá»›i Langfuse Ä‘á»ƒ trace token usage.

## ğŸš€ TÃ­nh nÄƒng

- **2 API riÃªng biá»‡t**: Má»—i API cháº¡y trÃªn 1 GPU riÃªng
- **Qwen 2.5 7B**: Model Ä‘Æ°á»£c tá»‘i Æ°u vá»›i GPU memory utilization 0.7
- **Langfuse Integration**: Tá»± Ä‘á»™ng trace token usage vÃ  request/response
- **Docker Compose**: Dá»… dÃ ng deploy vÃ  quáº£n lÃ½
- **Local Langfuse**: Cháº¡y Langfuse locally Ä‘á»ƒ quáº£n lÃ½ traces

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- Docker vÃ  Docker Compose
- NVIDIA GPU vá»›i CUDA support
- 2 GPU (má»—i GPU cho 1 API)
- NVIDIA Container Toolkit

## ğŸ› ï¸ Setup

### 1. Clone vÃ  cÃ i Ä‘áº·t

```bash
git clone <your-repo>
cd <your-repo>
```

### 2. Cáº¥u hÃ¬nh environment

```bash
cp .env.example .env
```

Chá»‰nh sá»­a file `.env` vá»›i thÃ´ng tin Langfuse cá»§a báº¡n:

```env
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_HOST=https://cloud.langfuse.com
```

### 3. Khá»Ÿi cháº¡y há»‡ thá»‘ng

```bash
docker-compose up -d
```

Há»‡ thá»‘ng sáº½:

- Táº£i model Qwen 2.5 7B lÃªn 2 GPU riÃªng biá»‡t
- Khá»Ÿi cháº¡y 2 API server trÃªn port 8000 vÃ  8001
- Khá»Ÿi cháº¡y Langfuse dashboard trÃªn port 3000

## ğŸ”— API Endpoints

### API 1 (GPU 0): http://localhost:8000

### API 2 (GPU 1): http://localhost:8001

### Health Check

```bash
curl http://localhost:8000/health
curl http://localhost:8001/health
```

### Chat API

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "max_tokens": 100,
    "temperature": 0.7,
    "trace_id": "my-trace-id"
  }'
```

### Generate API

```bash
curl -X POST "http://localhost:8000/generate?prompt=Hello&max_tokens=50&temperature=0.7"
```

## ğŸ“Š Langfuse Dashboard

Truy cáº­p Langfuse dashboard táº¡i: http://localhost:3000

Táº¡i Ä‘Ã¢y báº¡n cÃ³ thá»ƒ:

- Xem táº¥t cáº£ traces tá»« cáº£ 2 API
- Theo dÃµi token usage
- PhÃ¢n tÃ­ch performance
- Filter theo project (project-1, project-2)

## ğŸ§ª Test

Cháº¡y test client Ä‘á»ƒ kiá»ƒm tra há»‡ thá»‘ng:

```bash
python test_client.py
```

## ğŸ“ Cáº¥u trÃºc project

```
.
â”œâ”€â”€ docker-compose.yml          # Docker Compose configuration
â”œâ”€â”€ Dockerfile.vllm            # Dockerfile cho vLLM container
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py               # vLLM API server vá»›i Langfuse
â”œâ”€â”€ test_client.py            # Test script
â””â”€â”€ README.md                 # Documentation
```

## ğŸ”§ Cáº¥u hÃ¬nh

### GPU Memory Utilization

Chá»‰nh sá»­a `GPU_MEMORY_UTILIZATION` trong docker-compose.yml hoáº·c .env file:

```yaml
environment:
  - GPU_MEMORY_UTILIZATION=0.7 # Sá»­ dá»¥ng 70% GPU memory
```

### Model Configuration

Thay Ä‘á»•i model trong docker-compose.yml:

```yaml
environment:
  - MODEL_NAME=Qwen/Qwen2.5-7B-Instruct
```

### Project Names

Má»—i API cÃ³ project name riÃªng Ä‘á»ƒ phÃ¢n biá»‡t trong Langfuse:

```yaml
environment:
  - PROJECT_NAME=project-1 # Cho API 1
  - PROJECT_NAME=project-2 # Cho API 2
```

## ğŸš¨ Troubleshooting

### GPU khÃ´ng Ä‘Æ°á»£c nháº­n

```bash
# Kiá»ƒm tra NVIDIA Container Toolkit
nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.1-base-ubuntu22.04 nvidia-smi
```

### Model loading cháº­m

- Láº§n Ä‘áº§u sáº½ táº£i model tá»« HuggingFace (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)
- Model sáº½ Ä‘Æ°á»£c cache trong volume

### Langfuse connection error

- Kiá»ƒm tra LANGFUSE_PUBLIC_KEY vÃ  LANGFUSE_SECRET_KEY
- Äáº£m báº£o internet connection Ä‘á»ƒ káº¿t ná»‘i Langfuse cloud

## ğŸ“ˆ Monitoring

### Logs

```bash
# Xem logs cá»§a táº¥t cáº£ services
docker-compose logs -f

# Xem logs cá»§a specific service
docker-compose logs -f vllm-api-1
docker-compose logs -f vllm-api-2
```

### GPU Usage

```bash
nvidia-smi
```

### API Status

```bash
curl http://localhost:8000/health
curl http://localhost:8001/health
```

## ğŸ”„ Restart Services

```bash
# Restart táº¥t cáº£
docker-compose restart

# Restart specific service
docker-compose restart vllm-api-1
docker-compose restart vllm-api-2
```

## ğŸ›‘ Stop Services

```bash
docker-compose down
```

## ğŸ“ Notes

- Má»—i API cháº¡y Ä‘á»™c láº­p trÃªn 1 GPU riÃªng
- Token usage Ä‘Æ°á»£c track tá»± Ä‘á»™ng qua Langfuse
- Model Ä‘Æ°á»£c cache Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ load láº§n sau
- CÃ³ thá»ƒ scale thÃªm API báº±ng cÃ¡ch thÃªm service trong docker-compose.yml
